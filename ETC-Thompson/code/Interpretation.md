For the pure exploration algorithm (1), the cumulative regret looks linear with respect to the number of iterations. This is because the algorithm is not taking into account the information it has about the arms and is just exploring the arms randomly, so on average in our $30$ runs of the algorithm, the expected cumulative regret is linear with the number of iterations as at each step we are losing $\mu_{max} - \bar{\mu}$ on average, where $\mu_{max}$ is the maximum mean of the arms and $\bar{\mu}$ is the mean of the arms. Looking at its variance, this seems to almost always be the worst performing algorithm, since it never learns anything and always has a linear regret in all runs (high exploration and no exploitation).

Since the greedy algorithm (2) explores the 5 arms for the first 5 rounds, and it then commits to the best arm, then each run of the algorithm will have a linear tail (as you can see in the graph, the variance of the regret is is shooting in an exactly linear tail, which makes sense because we stick to the same arm for the rest of the rounds after the 5th). Over the 30 runs, the expected cumulative regret is also linear with the number of iterations because it is the average of linear tails. This algorithm seems to have a very good expected value (beating almost all algorithms except Thomopson Sampling and Gittins Index) in the Bernoulli bandits case. But this is because the reward distributions are simple. Also, it has the highest variance and its boundaries are ridiculously spread out since it at some runs always commits to the worst arm possible and at other runs always commits to the best arm possible. This also has extremely low exploration but extremely high exploitation, which is why it is not the best performing in theory or in practice.

For the Explore-Then-Commit algorithm (3), the cumulative regret with the optimal choice of $N_e$ is indeed not looking linear with respect to the number of iterations, which matches what we talked about in lecture. Although after a certain iteration ($kN_e=5\times 10=50$), the trend in the cumulative regret changes and becomes linear, but over all the $T=100$ rounds, the cumulative regret is not linear and is more like the suggested lecture bound of $O(T^\frac{2}{3})$. This algorithm seems to be in between algorithm (1) and (2) in terms of performance on expected value. This algorithm has a decent balance between exploration and exploitation, and it is not as bad as algorithm (2) in terms of variance.

For the $\epsilon$-Greedy algorithm (4), the UCB algorithm (5), and the Thompson Sampling algorithm (6), the cumulative regret is sublinear with respect to the number of iterations (which aligns with the regret bounds mentioned in lecture). In addition, the upper boundary of the confidence interval of the cumulative regret is not only non-linear, but also is noisy, which makes sense since any run of these algorithms will allow (for the most part, even in Thompson Sampling) exploration throughout the whole run. So, these algorithms seem to have a good balance between exploration and exploitation. Lastly, algorithms (4) and (5) perform relatively okay in terms of expected value, and the Thompson Sampling algorithm (6) performs the best out of the three in terms of expected value, and is also one of the best performing algorithms overall both in expectation and even when including the possible variance.

For the Gittins Index algorithm (7), the cumulative regret seems to be sublinear as well. This is because the algorithm is not only exploring, but also exploiting, and it is exploiting the best arm it has found so far. This algorithm seems to have a good balance between exploration and exploitation, as it seems to mimic the Thompson Sampling algorithm (6) in terms of logic. As for the variance, once it picks a "wrong" arm, it accumulates linearly until the index of this arm is not maximal anymore (which excuses why the tails look stepwise linear). Lastly, this algorithm seems to have a good balance between exploration and exploitation, and it seems to be the best performer in terms of expected value overall.